{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2nUZU-qFC-h",
        "outputId": "d1008ef2-a4df-43a8-a6ee-d6ed45155147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 39 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 52.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=de656b63e8371a14717e0dfcaacf842ce29b97f00765543a796a84552ca53b81\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ],
      "source": [
        "# install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La5H78shM2Qa"
      },
      "source": [
        "Retail Data Analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-aOaL_gM4om"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN-v0RJVM4PW"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTd4gqFHDfJQ"
      },
      "outputs": [],
      "source": [
        "df = spark.read.option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").csv(\"retail-data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzKap3uiDi-l"
      },
      "outputs": [],
      "source": [
        "customerAmount = df.select(col(\"_c3\").alias(\"CustomerID\"),col(\"_c2\").alias(\"Amount\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4pQs9f1Djde"
      },
      "outputs": [],
      "source": [
        "results = customerAmount.groupBy(\"CustomerID\").avg(\"Amount\").withColumnRenamed(\"avg(Amount)\", \"AvgAmount\").orderBy(\"CustomerID\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv-zTsIrDm78"
      },
      "outputs": [],
      "source": [
        "results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9k4501eKf0Y"
      },
      "source": [
        "Social Media Content Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvdVbSOsBGcf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit, col, explode, arrays_zip\n",
        "from pyspark.sql.types import ArrayType, StructType\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddHMrLlmeJ4i"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI345tAmTQs_"
      },
      "outputs": [],
      "source": [
        "akunTwitter = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"AkunTwitter_POS.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBor8qmiWrZJ"
      },
      "outputs": [],
      "source": [
        "hashtagTwitter = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"HashtagTwitter_POS.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR0bQtcwWrPN"
      },
      "outputs": [],
      "source": [
        "instagram = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").json(\"Instagram_POS.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFNqrL_UaHVw"
      },
      "outputs": [],
      "source": [
        "akun = akunTwitter.select(\"username\", col(\"tweet\").alias(\"content\")).withColumn(\"source\", lit(\"Twitter\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdQh0yt4hZ6L"
      },
      "outputs": [],
      "source": [
        "hashtag = hashtagTwitter.select(\"username\", col(\"tweet\").alias(\"content\")).withColumn(\"source\", lit(\"Twitter\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDGot7dUhZ3P"
      },
      "outputs": [],
      "source": [
        "ig = instagram.select(\"caption\",\"comments.author\",\"comments.comment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZT26Z19GfMN"
      },
      "outputs": [],
      "source": [
        "igAuthorComment = ig.withColumn(\"comments\", arrays_zip(\"author\", \"comment\")) \\\n",
        ".withColumn(\"comments\", explode(\"comments\")) \\\n",
        ".select(col(\"comments.author\").alias(\"username\"), col(\"comments.comment\").alias(\"content\")) \\\n",
        ".withColumn(\"source\", lit(\"Instagram\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkVk0WS29Ny2"
      },
      "outputs": [],
      "source": [
        "joinAll = akun.unionAll(hashtag).unionAll(igAuthorComment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKn6w7QxNDwp"
      },
      "outputs": [],
      "source": [
        "joinAll.coalesce(1).write.option(\"header\", \"true\").csv(\"joinAll.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ahyfxUDRuld"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9R81-piFAwM"
      },
      "source": [
        "SparkSQL Data Analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVWVrjy3E9os"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTwvLQRZGr0f"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L_0FwQsFKR8"
      },
      "outputs": [],
      "source": [
        "schema = StructType([ \\\n",
        "                     StructField(\"InvoiceNo\", IntegerType(), True), \\\n",
        "                     StructField(\"StockCode\", IntegerType(), True), \\\n",
        "                     StructField(\"Description\", StringType(), True), \\\n",
        "                     StructField(\"Quantity\", IntegerType(), True), \\\n",
        "                     StructField(\"InvoiceData\", StringType(), True), \\\n",
        "                     StructField(\"Amount\", FloatType(), True), \\\n",
        "                     StructField(\"CustomerID\", FloatType(), True), \\\n",
        "                     StructField(\"Country\", StringType(), True)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPdZ7KIfFNE3"
      },
      "outputs": [],
      "source": [
        "df = spark.read.schema(schema).option(\"delimiter\", \";\").csv(\"retail-data-full.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy3bumJvFO5z"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"retail_data_full\")\n",
        "results = spark.sql(\"SELECT CustomerID, min(Amount) FROM retail_data_full GROUP BY CustomerID ORDER BY CustomerID\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlZD9iZ9Kqpt"
      },
      "outputs": [],
      "source": [
        "for result in results:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcvOfjQOFXPW"
      },
      "source": [
        "OR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSqo7tfTFYpr"
      },
      "outputs": [],
      "source": [
        "minAmount = df.select(\"CustomerID\", \"Amount\")\n",
        "results = minAmount.groupBy(\"CustomerID\").min(\"Amount\").sort(\"CustomerID\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VHpSdn7HgD9"
      },
      "outputs": [],
      "source": [
        "for result in results:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiFZL61DLxSL"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Week_12_Homework.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
